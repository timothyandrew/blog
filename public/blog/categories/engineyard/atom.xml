<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: engineyard | Timothy's Blog]]></title>
  <link href="http://blog.timothyandrew.net/blog/categories/engineyard/atom.xml" rel="self"/>
  <link href="http://blog.timothyandrew.net/"/>
  <updated>2013-04-28T08:21:51+05:30</updated>
  <id>http://blog.timothyandrew.net/</id>
  <author>
    <name><![CDATA[Timothy Andrew]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Changing the Server Timeout on EngineYard]]></title>
    <link href="http://blog.timothyandrew.net/blog/2013/04/12/changing-the-server-timeout-on-engineyard/"/>
    <updated>2013-04-12T22:29:00+05:30</updated>
    <id>http://blog.timothyandrew.net/blog/2013/04/12/changing-the-server-timeout-on-engineyard</id>
    <content type="html"><![CDATA[<p>While working on <a href="http://github.com/c42/survey-web">survey-web</a> today, we were stuck for a really long time trying to figure out this problem.</p>

<p>Unless otherwise specified, image uploads while adding a response are capped at 5MB per image.
Adding a larger image (like this 20MB image) should result in a validation error showing up.</p>

<p><img src="/images/2013-04-12-image-too-big.png" alt="Validation Error" /></p>

<p>On production, we'd see this.</p>

<p><img src="/images/2013-04-12-502.png" alt="Production" /></p>

<p>After a <em>lot</em> of digging, including looking at Carrierwave (and <a href="https://github.com/lardawge/carrierwave_backgrounder">Backgrounder</a>), delayed_job server logs, and our controller logic pretty closely, we noticed in <code>production.log</code> that Rails was sending down a <code>200</code>, but the browser was recieving a <code>502</code>.</p>

<p><code>unicorn.log</code> showed that a worker process was being killed with a <code>SIGIOP</code> whenever the error page showed up.</p>

<p>Only then did we realise that the worker was being killed around 60s every time. It had to be a timeout issue.</p>

<p>On EngineYard, the unicorn config already had:</p>

<p>```ruby</p>

<h1>sets the timeout of worker processes to +seconds+.  Workers</h1>

<h1>handling the request/app.call/response cycle taking longer than</h1>

<h1>this time period will be forcibly killed (via SIGKILL).  This</h1>

<h1>timeout is enforced by the master process itself and not subject</h1>

<h1>to the scheduling limitations by the worker process.  Due the</h1>

<h1>low-complexity, low-overhead implementation, timeouts of less</h1>

<h1>than 3.0 seconds can be considered inaccurate and unsafe.</h1>

<p>timeout 180
```
The server didn't seem to be following this configuration.</p>

<p>After a fair bit of googling and help from the <code>#engineyard</code> IRC channel, this is what we did to fix it.<br/>
Add the following lines to <code>/data/nginx/nginx.conf</code> inside the <code>http{}</code> block (replacing 300 with the timeout you need).</p>

<p><code>nginx
client_header_timeout 300;
client_body_timeout 300;
send_timeout 300;
</code></p>

<p>And restart nginx/unicorn with</p>

<p><code>bash
$ sudo /etc/init.d/nginx reload
$ /engineyard/bin/app_&lt;app_name&gt; reload
</code></p>
]]></content>
  </entry>
  
</feed>
